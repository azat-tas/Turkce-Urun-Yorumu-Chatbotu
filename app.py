# =================================================================================
#                 TÃ¼rkÃ§e ÃœrÃ¼n YorumlarÄ±na DayalÄ± RAG Chatbot
# =================================================================================
# Bu Streamlit uygulamasÄ±, FAISS ve Google Gemini kullanarak Ã¼rÃ¼n yorumlarÄ±na
# dayalÄ± sorularÄ± yanÄ±tlayan bir chatbot sunar.

# --- Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ± ---
import streamlit as st                 # Web arayÃ¼zÃ¼nÃ¼ oluÅŸturmak iÃ§in (UI Framework)
import google.generativeai as genai    # Google Gemini modelini kullanmak iÃ§in (LLM API)
import os                              # Ortam deÄŸiÅŸkenlerini okumak iÃ§in (API AnahtarÄ±)
import faiss                           # VektÃ¶r veritabanÄ± oluÅŸturma ve arama iÃ§in (Vector DB)
import pickle                          # Python nesnelerini (meta veri listesi) kaydetmek/yÃ¼klemek iÃ§in
import numpy as np                     # SayÄ±sal iÅŸlemler ve diziler iÃ§in (FAISS iÃ§in gerekli)
from sentence_transformers import SentenceTransformer # Metinleri vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in (Embedding Modeli)

# --- AdÄ±m 1: YapÄ±landÄ±rma ve Kaynak YÃ¼kleme ---

# @st.cache_resource: Bu fonksiyonlarÄ±n sonuÃ§larÄ± cache'lenir, 
# bÃ¶ylece uygulama her yenilendiÄŸinde tekrar Ã§alÄ±ÅŸtÄ±rÄ±lmazlar. Bu, hÄ±z ve verimlilik saÄŸlar.
@st.cache_resource 
def load_api_key_and_configure_gemini():
    """Google API AnahtarÄ±nÄ± HF Secrets'tan (ortam deÄŸiÅŸkeni) yÃ¼kler ve Gemini modelini yapÄ±landÄ±rÄ±r."""
    try:
        # 1. API AnahtarÄ±nÄ± Oku: Hugging Face Secrets'a eklenen anahtarÄ± ortam deÄŸiÅŸkeni olarak okur.
        api_key = os.environ.get('GOOGLE_API_KEY') 
        if api_key is None:
            st.error("HATA: Google API AnahtarÄ± Hugging Face Secrets'ta bulunamadÄ±. LÃ¼tfen Space > Settings > Secrets bÃ¶lÃ¼mÃ¼nÃ¼ kontrol edin.")
            st.stop()
            
        # 2. Gemini KÃ¼tÃ¼phanesini YapÄ±landÄ±r: AlÄ±nan API anahtarÄ± ile google.generativeai kÃ¼tÃ¼phanesini ayarlar.
        genai.configure(api_key=api_key)
        
        # 3. Gemini Modelini YÃ¼kle: KullanÄ±lacak LLM modelini (Ã¶rn: 'gemini-flash-latest') hazÄ±rlar.
        model = genai.GenerativeModel('gemini-flash-latest')
        
        st.success("âœ… Google API AnahtarÄ± yÃ¼klendi ve Gemini modeli yapÄ±landÄ±rÄ±ldÄ±.")
        # 4. Modeli DÃ¶ndÃ¼r: YapÄ±landÄ±rÄ±lmÄ±ÅŸ modeli ana uygulamada kullanmak Ã¼zere dÃ¶ndÃ¼rÃ¼r.
        return model
    except Exception as e:
        st.error(f"HATA: API AnahtarÄ± veya Gemini yapÄ±landÄ±rmasÄ± baÅŸarÄ±sÄ±z: {e}")
        st.stop()

@st.cache_resource
def load_retrieval_system():
    """Embedding modelini, FAISS indeksini ve meta veriyi diskten yÃ¼kler."""
    try:
        st.info("â³ Arama sistemi (Embedding Modeli, FAISS, Meta Veri) yÃ¼kleniyor...")
        
        # 1. Embedding Modelini YÃ¼kle: Metinleri ve sorgularÄ± vektÃ¶re Ã§evirecek modeli yÃ¼kler.
        embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
        
        # 2. FAISS Ä°ndeksini YÃ¼kle: Ã–nceden hesaplanmÄ±ÅŸ vektÃ¶rleri iÃ§eren veritabanÄ±nÄ± yÃ¼kler.
        index = faiss.read_index("vektor_indeksi.faiss")
        
        # 3. Meta Veriyi YÃ¼kle: Hangi vektÃ¶rÃ¼n hangi yoruma ait olduÄŸunu belirten listeyi yÃ¼kler.
        with open("meta_veri.pkl", 'rb') as f:
            processed_data = pickle.load(f)
            
        st.success("âœ… Arama sistemi baÅŸarÄ±yla yÃ¼klendi.")
        # 4. YÃ¼klenen BileÅŸenleri DÃ¶ndÃ¼r: Modeli, indeksi ve meta veriyi ana uygulamada kullanmak Ã¼zere dÃ¶ndÃ¼rÃ¼r.
        return embedding_model, index, processed_data
    except FileNotFoundError:
        st.error("HATA: 'vektor_indeksi.faiss' veya 'meta_veri.pkl' dosyalarÄ± bulunamadÄ±! LÃ¼tfen bu dosyalarÄ±n Space'te mevcut olduÄŸundan emin olun.")
        st.stop()
    except Exception as e:
        st.error(f"HATA: Arama sistemi yÃ¼klenirken hata: {e}")
        st.stop()

# --- AdÄ±m 2: RAG Pipeline FonksiyonlarÄ± ---

# Global deÄŸiÅŸkenler (load_retrieval_system tarafÄ±ndan doldurulacak)
embedding_model = None
index = None
processed_data = None

def retrieve_contexts(query, k=10): 
    """Verilen sorgu iÃ§in FAISS veritabanÄ±ndan en ilgili 'k' metin parÃ§asÄ±nÄ± (chunk) bulur."""
    if embedding_model is None or index is None or processed_data is None:
        return [] 
        
    query_vector = embedding_model.encode([query]) 
    try:
        distances, indices = index.search(np.array(query_vector).astype('float32'), k) 
        contexts = [processed_data[idx]['text'] for idx in indices[0]] 
        return contexts
    except Exception as e:
        st.error(f"FAISS aramasÄ± sÄ±rasÄ±nda hata: {e}")
        return []

def answer_with_rag(query, generation_model_from_cache):
    """Retrieval (Arama) ve Generation (Ãœretim) adÄ±mlarÄ±nÄ± birleÅŸtirir."""
    
    # 1. Retrieval: Ä°lgili yorum parÃ§alarÄ±nÄ± bul
    relevant_contexts = retrieve_contexts(query)
    if not relevant_contexts:
        return "Elimdeki yorumlarda bu konuyla ilgili bilgi bulmakta zorlanÄ±yorum."
        
    context_str = "\n\n---\n\n".join(relevant_contexts)
    
    # 2. Generation: Gemini iÃ§in prompt'u oluÅŸtur
    prompt = f"""Sen, TÃ¼rkÃ§e Ã¼rÃ¼n yorumlarÄ±na dayanarak Ã¼rÃ¼nler hakkÄ±nda bilgi veren ve tavsiyelerde bulunan bir asistansÄ±n. 
GÃ¶revin, aÅŸaÄŸÄ±da saÄŸlanan Ã¼rÃ¼n yorumu parÃ§alarÄ±nÄ± (BaÄŸlam) dikkatlice analiz etmek ve sorulan soruya en mantÄ±klÄ± ve genel eÄŸilimi yansÄ±tan cevabÄ± oluÅŸturmaktÄ±r.
EÄŸer baÄŸlamda sorunun cevabÄ± doÄŸrudan geÃ§miyorsa, verilen bilgileri yorumlayarak, Ã§Ä±karÄ±m yaparak veya genel bir Ã¶zet sunarak soruyu yanÄ±tla.
CevabÄ±nÄ± her zaman kibar ve yardÄ±mcÄ± bir tonda tut.

BAÄLAM:
{context_str}

SORU:
{query}

CEVAP:"""
    
    # Gemini'den cevap Ã¼retmesini iste
    try:
        response = generation_model_from_cache.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(temperature=0.7) 
        )
        # GÃ¼venlik filtrelerini kontrol et
        if not response.parts:
             return "âš ï¸ Ãœretilen cevap gÃ¼venlik filtrelerine takÄ±ldÄ±. LÃ¼tfen sorunuzu deÄŸiÅŸtirin."
        return response.text
    except Exception as e:
        st.error(f"HATA: Gemini'den cevap Ã¼retilirken sorun oluÅŸtu: {e}")
        return "Cevap Ã¼retirken bir hatayla karÅŸÄ±laÅŸtÄ±m."

# --- AdÄ±m 3: Streamlit ArayÃ¼zÃ¼nÃ¼ OluÅŸturma ve Ã‡alÄ±ÅŸtÄ±rma ---

def main():
    """Streamlit arayÃ¼zÃ¼nÃ¼ oluÅŸturur ve chatbot mantÄ±ÄŸÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±r."""
    
    st.set_page_config(page_title="ÃœrÃ¼n Yorum Chatbot'u", page_icon="ğŸ“¦")
    st.title("ğŸ“¦ TÃ¼rkÃ§e ÃœrÃ¼n Yorum Chatbot'u")
    st.caption("Veri Seti: Turkish Product Reviews (Alt KÃ¼me) | Model: Gemini & Sentence Transformers")

    # Uygulama baÅŸlarken gerekli modelleri ve veriyi yÃ¼kle
    generation_model = load_api_key_and_configure_gemini()
    global embedding_model, index, processed_data 
    embedding_model, index, processed_data = load_retrieval_system()

    # Sohbet geÃ§miÅŸini tut
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # GeÃ§miÅŸ mesajlarÄ± yazdÄ±r
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # KullanÄ±cÄ±dan yeni girdi al
    if user_prompt := st.chat_input("Bir Ã¼rÃ¼n veya Ã¶zellik hakkÄ±nda soru sorun..."):
        st.session_state.messages.append({"role": "user", "content": user_prompt})
        with st.chat_message("user"):
            st.markdown(user_prompt)

        # Chatbot'un cevabÄ±nÄ± al ve gÃ¶ster
        with st.chat_message("assistant"):
            with st.spinner("Yorumlar taranÄ±yor ve cevap oluÅŸturuluyor..."):
                response = answer_with_rag(user_prompt, generation_model)
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})

# Python script'i doÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda main() fonksiyonunu Ã§aÄŸÄ±r
if __name__ == "__main__":
    main()
